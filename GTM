# Install required packages if not already installed:
# pip install torch transformers tensorflow

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Load GPT-2 model and tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Function to generate text using GPT-2
def generate_text(prompt, max_length=100):
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Generate text from GPT-2
user_prompt = "The future of technology is"
generated_text = generate_text(user_prompt)
print("Generated Text:\n", generated_text)

# Create and compile LSTM model
def create_lstm_model(vocab_size, embedding_dim, hidden_units):
    model = keras.Sequential([
        layers.Embedding(vocab_size, embedding_dim),
        layers.LSTM(hidden_units, return_sequences=True),
        layers.LSTM(hidden_units),
        layers.Dense(vocab_size, activation='softmax')
    ])
    return model

vocab_size = 10000
embedding_dim = 256
hidden_units = 512

lstm_model = create_lstm_model(vocab_size, embedding_dim, hidden_units)
lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Print model summary
lstm_model.summary()
